{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CSH\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Conv2D, Flatten,MaxPooling2D,BatchNormalization,Lambda, AveragePooling2D, Dropout, SpatialDropout2D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('/kaggle/input/dacon-crash-prediction/train_features.csv')\n",
    "test = pd.read_csv('/kaggle/input/dacon-crash-prediction/test_features.csv')\n",
    "train_target = pd.read_csv('/kaggle/input/dacon-crash-prediction/train_target.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def JB(y):\n",
    "    result = []\n",
    "    for i, value in enumerate(y):\n",
    "        if i == 0:\n",
    "            result.append(value)\n",
    "            continue\n",
    "        result.append(result[-1] + value)\n",
    "    return result\n",
    "\n",
    "def JB_applying(train):\n",
    "    for S in ['S1', 'S2', 'S3', 'S4']:\n",
    "        train[S + '_JB'] = 0\n",
    "        for id in train['id'].unique():\n",
    "            values = train[S][train['id']==id]\n",
    "            train[S + '_JB'][train['id']==id] = JB(values)\n",
    "    return train\n",
    "\n",
    "def JB2_applying(train):\n",
    "    for S in ['S1', 'S2', 'S3', 'S4']:\n",
    "        train[S + '_JB2'] = 0\n",
    "        for id in train['id'].unique():\n",
    "            values = train[S + '_JB'][train['id']==id]\n",
    "            train[S + '_JB2'][train['id']==id] = JB(values)\n",
    "    return train\n",
    "\n",
    "def JB3_applying(train):\n",
    "    for S in ['S1', 'S2', 'S3', 'S4']:\n",
    "        train[S + '_JB3'] = 0\n",
    "        for id in train['id'].unique():\n",
    "            values = train[S + '_JB2'][train['id']==id]\n",
    "            train[S + '_JB3'][train['id']==id] = JB(values)\n",
    "    return train\n",
    "\n",
    "def new_var(train):\n",
    "    train['S1-S2'] = train['S1'] - train['S2']\n",
    "    train['S3-S4'] = train['S3'] - train['S4']\n",
    "    return train\n",
    "\n",
    "def arr_time(train):\n",
    "    for S in ['S1', 'S2', 'S3', 'S4']:\n",
    "        values = []\n",
    "        for id_num in sorted(train['id'].unique()):\n",
    "            arr_time = 375 - (train[train['id']==id_num][S]!=0).sum()\n",
    "            values += [0]*(arr_time-1) + [1]*(376-arr_time)\n",
    "        train[S +'_AT'] = values\n",
    "    return train\n",
    "\n",
    "def AT_diff(train):\n",
    "    train['S1-S2_AT'] = train['S1_AT'] - train['S2_AT']\n",
    "    train['S3-S4_AT'] = train['S3_AT'] - train['S4_AT']\n",
    "    return train\n",
    "\n",
    "def getting_convex(train, id, S, desc=True, is_abs=False):\n",
    "    vex = []\n",
    "    prev_value = 0\n",
    "    prev_trend = 0\n",
    "    trend = 0\n",
    "    for idx, value in enumerate(train[train['id']==id][S]):\n",
    "        diff = value- prev_value\n",
    "        if diff > 0:\n",
    "            trend = 1\n",
    "        elif diff == 0:\n",
    "            trend = 0\n",
    "            if prev_trend !=0:\n",
    "                vex.append((prev_value, idx))\n",
    "        else: \n",
    "            trend = -1\n",
    "\n",
    "        if trend * prev_trend < 0 :\n",
    "            if is_abs:\n",
    "                vex.append((abs(prev_value), idx))\n",
    "            else:\n",
    "                vex.append((prev_value, idx))\n",
    "        prev_value = value\n",
    "        prev_trend = trend\n",
    "\n",
    "    result = sorted(vex, key= lambda x: -x[0])\n",
    "    if desc == False:\n",
    "        result = sorted(vex, key= lambda x: x[0])\n",
    "    return result\n",
    "\n",
    "\n",
    "def checking_convex(train):\n",
    "    for S in ['S1', 'S2', 'S3', 'S4']:\n",
    "        value = []\n",
    "        for id in train['id'].unique():\n",
    "            tmp = np.array([0]*375)\n",
    "            tmp[np.array(list(map(lambda x:x[1], getting_convex(train, id, S))))] = 1\n",
    "            value += list(tmp)\n",
    "        train[S +'_convex'] = value\n",
    "    return train\n",
    "\n",
    "def checking_convex_JB(train):\n",
    "    for S in ['S1_JB', 'S2_JB', 'S3_JB', 'S4_JB']:\n",
    "        value = []\n",
    "        for id in train['id'].unique():\n",
    "            tmp = np.array([0]*375)\n",
    "            tmp[np.array(list(map(lambda x:x[1], getting_convex(train, id, S))))] = 1\n",
    "            value += list(tmp)\n",
    "        train[S +'_convex'] = value\n",
    "    return train\n",
    "\n",
    "\n",
    "def getting_conflict(train, id, S): # 이 지점을 다 1로? \n",
    "    result = []\n",
    "    prev = 0\n",
    "    for value, t, in sorted(getting_convex(train, id, S), key=lambda x: x[1]):\n",
    "        if prev * value >0:\n",
    "            result.append((t, value))\n",
    "        prev = value\n",
    "    return result\n",
    "\n",
    "def checking_conflict(train):\n",
    "    for S in ['S1', 'S2', 'S3', 'S4']:\n",
    "        value = []\n",
    "        for id in train['id'].unique():\n",
    "            idx = list(map(lambda x:x[0], getting_conflict(train, id, S)))\n",
    "            if idx == []:\n",
    "                tmp = [0]*375\n",
    "            else:\n",
    "                tmp = np.array([0]*375)\n",
    "                tmp[np.array(idx)] = 1\n",
    "            value += list(tmp)\n",
    "        train[S +'_conflict'] = value\n",
    "    return train\n",
    "\n",
    "\n",
    "def first_conflict_time(train): \n",
    "    for S in ['S1', 'S2', 'S3', 'S4']:\n",
    "        value = []\n",
    "        for id in train['id'].unique():\n",
    "            conflicts = getting_conflict(train, id, S) \n",
    "            if conflicts == []:\n",
    "                value += [1]*375\n",
    "            else:\n",
    "                value += [1]*(conflicts[0][0]-1) + [0]*(376 - conflicts[0][0])\n",
    "        train[S +'_Fir_Conf_T'] = value\n",
    "    return train\n",
    "\n",
    "def getting_trend(train, id, S):\n",
    "    result = []\n",
    "    prev_value = 0\n",
    "    prev_trend = 0\n",
    "    trend = 0\n",
    "    for idx, value in enumerate(train[train['id']==id][S]):\n",
    "        diff = value- prev_value\n",
    "        if diff > 0:\n",
    "            trend = 1\n",
    "        elif diff == 0:\n",
    "            trend = 0\n",
    "        else: \n",
    "            trend = -1\n",
    "        result.append(trend)\n",
    "        prev_value = value\n",
    "        prev_trend = trend\n",
    "    return result\n",
    "\n",
    "def checking_trend(train):\n",
    "    for S in ['S1', 'S2', 'S3', 'S4']:\n",
    "        value = []\n",
    "        for id in train['id'].unique():\n",
    "            value += getting_trend(train, id, S)\n",
    "        train[S + '_trend'] = value\n",
    "    return train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "sample_submission = pd.read_csv('/kaggle/input/dacon-crash-prediction/sample_submission.csv')\n",
    "\n",
    "master = JB_applying(train)\n",
    "master = JB2_applying(master)\n",
    "master = arr_time(master)\n",
    "master = checking_convex(master)\n",
    "\n",
    "test_master = JB_applying(test)\n",
    "test_master = JB2_applying(test_master)\n",
    "test_master = arr_time(test_master)\n",
    "test_master = checking_convex(test_master)\n",
    "\n",
    "# reshape\n",
    "master_np = np.reshape(np.array(master)[:,1:], (2800, 375, master.shape[1]-1, 1)) # id 컬럼 생략\n",
    "test_master_np = np.reshape(np.array(test_master)[:,1:], (700, 375, test_master.shape[1]-1, 1))\n",
    "\n",
    "X_train = master_np.copy()\n",
    "Y_train = np.array(train_target)[:,1:]\n",
    "X_test = test_master_np.copy()\n",
    "\n",
    "print(X_train.shape, np.isnan(X_train).sum().sum())\n",
    "print(Y_train.shape, np.isnan(Y_train).sum().sum())\n",
    "print(X_test.shape, np.isnan(X_test).sum().sum())\n",
    "\n",
    "def my_loss(y_true, y_pred):\n",
    "    divResult = Lambda(lambda x: x[0]/x[1])([(y_pred-y_true),(y_true+0.000001)])\n",
    "    return K.mean(K.square(divResult))\n",
    "\n",
    "def my_loss_E1(y_true, y_pred):\n",
    "    return K.mean(K.square(y_true-y_pred)*np.array([1,1,0,0]))/2e+04\n",
    "\n",
    "def my_loss_E2_M(y_true, y_pred):\n",
    "    divResult = Lambda(lambda x: x[0]/x[1])([(y_pred-y_true),(y_true+0.000001)])\n",
    "    return K.mean(K.square(divResult)*np.array([0,0,1,0]))\n",
    "\n",
    "def my_loss_E2_V(y_true, y_pred):\n",
    "    divResult = Lambda(lambda x: x[0]/x[1])([(y_pred-y_true),(y_true+0.000001)])\n",
    "    return K.mean(K.square(divResult)*np.array([0,0,0,1]))\n",
    "\n",
    "def set_model(target_idx):  # 0:x,y, 1:m, 2:v\n",
    "    \n",
    "    activation = 'elu'\n",
    "    padding = 'valid'\n",
    "    model = Sequential()\n",
    "    nf = 16\n",
    "    fs = (3,1)\n",
    "    model.add(Conv2D(nf,fs, padding=padding, activation=activation,input_shape=(X_train.shape[1],X_train.shape[2],X_train.shape[3]))) # # of layers, filter size\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 1))) # 186, 13, 16\n",
    "\n",
    "    model.add(Conv2D(nf*2,fs, padding=padding, activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 1))) #  92, 13, 32\n",
    "\n",
    "    model.add(Conv2D(nf*4,fs, padding=padding, activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 1))) #  45, 13, 64\n",
    "\n",
    "    model.add(Conv2D(nf*8,fs, padding=padding, activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 1))) #  21, 13, 128\n",
    "\n",
    "    model.add(Conv2D(nf*16,fs, padding=padding, activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 1))) # 9, 13, 256\n",
    "\n",
    "    model.add(Conv2D(nf*32,fs, padding=padding, activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 1)))  # 3, 13, 512\n",
    "\n",
    "    \n",
    "    model.add(Flatten()) \n",
    "    model.add(Dense(128, activation ='elu')) # output size만 적어줌\n",
    "    model.add(Dense(64, activation ='elu'))\n",
    "    model.add(Dense(32, activation ='elu'))\n",
    "    model.add(Dense(16, activation ='elu'))\n",
    "    model.add(Dense(4))       \n",
    "\n",
    "    optimizer = keras.optimizers.Adam()\n",
    "\n",
    "    if target_idx==0:\n",
    "        model.compile(loss=my_loss_E1,\n",
    "                  optimizer=optimizer,\n",
    "                     )\n",
    "    elif target_idx==1:\n",
    "        model.compile(loss=my_loss_E2_M,\n",
    "                  optimizer=optimizer,\n",
    "                 )\n",
    "    else:\n",
    "        model.compile(loss=my_loss_E2_V,\n",
    "                  optimizer=optimizer,\n",
    "                 )\n",
    "    return model\n",
    "\n",
    "def train(model, X, Y, is_val=False):\n",
    "    MODEL_SAVE_FOLDER_PATH = './model/'\n",
    "    if not os.path.exists(MODEL_SAVE_FOLDER_PATH):\n",
    "        os.mkdir(MODEL_SAVE_FOLDER_PATH)\n",
    "\n",
    "    model_path = MODEL_SAVE_FOLDER_PATH + '{epoch:02d}-{val_loss:.4f}.hdf5'\n",
    "    best_save = ModelCheckpoint('best_m.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "\n",
    "    if is_val == False:\n",
    "        history = model.fit(X, Y,\n",
    "                      epochs=150,\n",
    "                      batch_size=256,\n",
    "                      shuffle=True,\n",
    "                      validation_split=0.2,\n",
    "                      verbose = 0,\n",
    "                      callbacks=[best_save])\n",
    "\n",
    "        fig, loss_ax = plt.subplots()\n",
    "        acc_ax = loss_ax.twinx()\n",
    "\n",
    "        loss_ax.plot(history.history['loss'], 'y', label='train loss')\n",
    "        loss_ax.plot(history.history['val_loss'], 'r', label='val loss')\n",
    "        loss_ax.set_xlabel('epoch')\n",
    "        loss_ax.set_ylabel('loss')\n",
    "        loss_ax.legend(loc='upper left')\n",
    "        plt.show()    \n",
    "        \n",
    "    else:\n",
    "        history = model.fit(X, Y,\n",
    "                      epochs=150,\n",
    "                      batch_size=256,\n",
    "                      shuffle=True,\n",
    "                      validation_split=0.2,\n",
    "                      verbose = 0,\n",
    "                      callbacks=[best_save])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def load_best_model(target_idx):\n",
    "\n",
    "    if target_idx == 0:\n",
    "        model = load_model('best_m.hdf5' , custom_objects={'my_loss_E1': my_loss, })\n",
    "    elif target_idx == 1:\n",
    "        model = load_model('best_m.hdf5' , custom_objects={'my_loss_E2_M': my_loss, })\n",
    "    else:\n",
    "        model = load_model('best_m.hdf5' , custom_objects={'my_loss_E2_V': my_loss, })\n",
    "\n",
    "    score = model.evaluate(X_train, Y_train, verbose=0) \n",
    "    print('loss:', score)\n",
    "\n",
    "    return model\n",
    "\n",
    "n_model = 35\n",
    "for i in range(n_model):\n",
    "    idx, _ = train_test_split(np.arange(2800), test_size=0.2)\n",
    "    X_train_bag = X_train[idx,:]\n",
    "    Y_train_bag = Y_train[idx,:]\n",
    "    \n",
    "    for target_idx in [0,1,2]: # 학습 순서 조정 \n",
    "        model = set_model(target_idx)\n",
    "        train(model, X_train_bag, Y_train_bag)    \n",
    "        best_model = load_best_model(target_idx)\n",
    "        pred = best_model.predict(X_test)\n",
    "\n",
    "        if target_idx == 0: # x, y 학습\n",
    "            sample_submission.iloc[:,1] += pred[:,0]\n",
    "            sample_submission.iloc[:,2] += pred[:,1]\n",
    "    #         X_train = np.concatenate([X_train, np.reshape(np.repeat(Y_train[:,0], 375), (2800,375,1,1))], axis=2) # X 설명변수로 추가\n",
    "    #         X_test = np.concatenate([X_test, np.reshape(np.repeat(pred[:,0], 375), (700,375,1,1))], axis=2)\n",
    "\n",
    "        elif target_idx == 1: # m 학습\n",
    "            sample_submission.iloc[:,3] += pred[:,2]\n",
    "\n",
    "        elif target_idx == 2: # v 학습\n",
    "            sample_submission.iloc[:,4] += pred[:,3]\n",
    "            \n",
    "sample_submission[['X', 'Y', 'M', 'V']] = sample_submission[['X', 'Y', 'M', 'V']]/n_model\n",
    "\n",
    "result1 = sample_submission.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "sample_submission = pd.read_csv('/kaggle/input/dacon-crash-prediction/sample_submission.csv')\n",
    "\n",
    "train = JB_applying(train); train = JB2_applying(train)\n",
    "master = JB_applying(train); master = JB2_applying(master)\n",
    "\n",
    "test = JB_applying(test); test = JB2_applying(test)\n",
    "test_master = JB_applying(test); test_master = JB2_applying(test_master)\n",
    "\n",
    "# reshape\n",
    "master_np = np.reshape(np.array(master)[:,1:], (2800, 375, master.shape[1]-1, 1)) # id 컬럼 생략\n",
    "test_master_np = np.reshape(np.array(test_master)[:,1:], (700, 375, test_master.shape[1]-1, 1))\n",
    "\n",
    "X_train = master_np.copy()\n",
    "Y_train = np.array(train_target)[:,1:]\n",
    "X_test = test_master_np.copy()\n",
    "\n",
    "print(X_train.shape, np.isnan(X_train).sum().sum())\n",
    "print(Y_train.shape, np.isnan(Y_train).sum().sum())\n",
    "print(X_test.shape, np.isnan(X_test).sum().sum())\n",
    "\n",
    "def my_loss(y_true, y_pred):\n",
    "    divResult = Lambda(lambda x: x[0]/x[1])([(y_pred-y_true),(y_true+0.000001)])\n",
    "    return K.mean(K.square(divResult))\n",
    "\n",
    "def my_loss_E1(y_true, y_pred):\n",
    "    return K.mean(K.square(y_true-y_pred)*np.array([1,1,0,0]))/2e+04\n",
    "\n",
    "def my_loss_E2_M(y_true, y_pred):\n",
    "    divResult = Lambda(lambda x: x[0]/x[1])([(y_pred-y_true),(y_true+0.000001)])\n",
    "    return K.mean(K.square(divResult)*np.array([0,0,1,0]))\n",
    "\n",
    "def my_loss_E2_V(y_true, y_pred):\n",
    "    divResult = Lambda(lambda x: x[0]/x[1])([(y_pred-y_true),(y_true+0.000001)])\n",
    "    return K.mean(K.square(divResult)*np.array([0,0,0,1]))\n",
    "\n",
    "def set_model(target_idx):  # 0:x,y, 1:m, 2:v\n",
    "    \n",
    "    activation = 'elu'\n",
    "    padding = 'valid'\n",
    "    model = Sequential()\n",
    "    nf = 16\n",
    "    fs = (3,1)\n",
    "    model.add(Conv2D(nf,fs, padding=padding, activation=activation,input_shape=(X_train.shape[1],X_train.shape[2],X_train.shape[3]))) # # of layers, filter size\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 1))) # 186, 13, 16\n",
    "\n",
    "    model.add(Conv2D(nf*2,fs, padding=padding, activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 1))) #  92, 13, 32\n",
    "\n",
    "    model.add(Conv2D(nf*4,fs, padding=padding, activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 1))) #  45, 13, 64\n",
    "\n",
    "    model.add(Conv2D(nf*8,fs, padding=padding, activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 1))) #  21, 13, 128\n",
    "\n",
    "    model.add(Conv2D(nf*16,fs, padding=padding, activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 1))) # 9, 13, 256\n",
    "\n",
    "    model.add(Conv2D(nf*32,fs, padding=padding, activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 1)))  # 3, 13, 512\n",
    "\n",
    "    \n",
    "    model.add(Flatten()) \n",
    "    model.add(Dense(128, activation ='elu')) # output size만 적어줌\n",
    "    model.add(Dense(64, activation ='elu'))\n",
    "    model.add(Dense(32, activation ='elu'))\n",
    "    model.add(Dense(16, activation ='elu'))\n",
    "    model.add(Dense(4))       \n",
    "\n",
    "    optimizer = keras.optimizers.Adam()\n",
    "\n",
    "    if target_idx==0:\n",
    "        model.compile(loss=my_loss_E1,\n",
    "                  optimizer=optimizer,\n",
    "                     )\n",
    "    elif target_idx==1:\n",
    "        model.compile(loss=my_loss_E2_M,\n",
    "                  optimizer=optimizer,\n",
    "                 )\n",
    "    else:\n",
    "        model.compile(loss=my_loss_E2_V,\n",
    "                  optimizer=optimizer,\n",
    "                 )\n",
    "    return model\n",
    "\n",
    "def train(model, X, Y, is_val=False):\n",
    "    MODEL_SAVE_FOLDER_PATH = './model/'\n",
    "    if not os.path.exists(MODEL_SAVE_FOLDER_PATH):\n",
    "        os.mkdir(MODEL_SAVE_FOLDER_PATH)\n",
    "\n",
    "    model_path = MODEL_SAVE_FOLDER_PATH + '{epoch:02d}-{val_loss:.4f}.hdf5'\n",
    "    best_save = ModelCheckpoint('best_m.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "\n",
    "    if is_val == False:\n",
    "        history = model.fit(X, Y,\n",
    "                      epochs=100,\n",
    "                      batch_size=256,\n",
    "                      shuffle=True,\n",
    "                      validation_split=0.2,\n",
    "                      verbose = 0,\n",
    "                      callbacks=[best_save])\n",
    "\n",
    "        fig, loss_ax = plt.subplots()\n",
    "        acc_ax = loss_ax.twinx()\n",
    "\n",
    "        loss_ax.plot(history.history['loss'], 'y', label='train loss')\n",
    "        loss_ax.plot(history.history['val_loss'], 'r', label='val loss')\n",
    "        loss_ax.set_xlabel('epoch')\n",
    "        loss_ax.set_ylabel('loss')\n",
    "        loss_ax.legend(loc='upper left')\n",
    "        plt.show()    \n",
    "        \n",
    "    else:\n",
    "        history = model.fit(X, Y,\n",
    "                      epochs=100,\n",
    "                      batch_size=256,\n",
    "                      shuffle=True,\n",
    "                      validation_split=0.2,\n",
    "                      verbose = 0,\n",
    "                      callbacks=[best_save])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def load_best_model(target_idx):\n",
    "\n",
    "    if target_idx == 0:\n",
    "        model = load_model('best_m.hdf5' , custom_objects={'my_loss_E1': my_loss, })\n",
    "    elif target_idx == 1:\n",
    "        model = load_model('best_m.hdf5' , custom_objects={'my_loss_E2_M': my_loss, })\n",
    "    else:\n",
    "        model = load_model('best_m.hdf5' , custom_objects={'my_loss_E2_V': my_loss, })\n",
    "\n",
    "    score = model.evaluate(X_train, Y_train, verbose=0) \n",
    "    print('loss:', score)\n",
    "\n",
    "    return model\n",
    "\n",
    "for target_idx in [0,1,2]: # 학습 순서 조정 \n",
    "    model = set_model(target_idx)\n",
    "    train(model,X_train, Y_train)    \n",
    "    best_model = load_best_model(target_idx)\n",
    "    pred = best_model.predict(X_test)\n",
    "    \n",
    "    if target_idx == 0: # x, y 학습\n",
    "        sample_submission.iloc[:,1] = pred[:,0]\n",
    "        sample_submission.iloc[:,2] = pred[:,1]\n",
    "#         X_train = np.concatenate([X_train, np.reshape(np.repeat(Y_train[:,0], 375), (2800,375,1,1))], axis=2) # X 설명변수로 추가\n",
    "#         X_test = np.concatenate([X_test, np.reshape(np.repeat(pred[:,0], 375), (700,375,1,1))], axis=2)\n",
    "\n",
    "    elif target_idx == 1: # m 학습\n",
    "        sample_submission.iloc[:,3] = pred[:,2]\n",
    "\n",
    "    elif target_idx == 2: # v 학습\n",
    "        sample_submission.iloc[:,4] = pred[:,3]\n",
    "\n",
    "result2 = sample_submission.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "sample_submission = pd.read_csv('/kaggle/input/dacon-crash-prediction/sample_submission.csv')\n",
    "\n",
    "master = JB_applying(train)\n",
    "master = JB2_applying(master)\n",
    "master = arr_time(master)\n",
    "\n",
    "test_master = JB_applying(test)\n",
    "test_master = JB2_applying(test_master)\n",
    "test_master = arr_time(test_master)\n",
    "\n",
    "# reshape\n",
    "master_np = np.reshape(np.array(master)[:,1:], (2800, 375, master.shape[1]-1, 1)) # id 컬럼 생략\n",
    "test_master_np = np.reshape(np.array(test_master)[:,1:], (700, 375, test_master.shape[1]-1, 1))\n",
    "\n",
    "X_train = master_np.copy()\n",
    "Y_train = np.array(train_target)[:,1:]\n",
    "X_test = test_master_np.copy()\n",
    "\n",
    "print(X_train.shape, np.isnan(X_train).sum().sum())\n",
    "print(Y_train.shape, np.isnan(Y_train).sum().sum())\n",
    "print(X_test.shape, np.isnan(X_test).sum().sum())\n",
    "\n",
    "def my_loss(y_true, y_pred):\n",
    "    divResult = Lambda(lambda x: x[0]/x[1])([(y_pred-y_true),(y_true+0.000001)])\n",
    "    return K.mean(K.square(divResult))\n",
    "\n",
    "def my_loss_E1(y_true, y_pred):\n",
    "    return K.mean(K.square(y_true-y_pred)*np.array([1,1,0,0]))/2e+04\n",
    "\n",
    "def my_loss_E2_M(y_true, y_pred):\n",
    "    divResult = Lambda(lambda x: x[0]/x[1])([(y_pred-y_true),(y_true+0.000001)])\n",
    "    return K.mean(K.square(divResult)*np.array([0,0,1,0]))\n",
    "\n",
    "def my_loss_E2_V(y_true, y_pred):\n",
    "    divResult = Lambda(lambda x: x[0]/x[1])([(y_pred-y_true),(y_true+0.000001)])\n",
    "    return K.mean(K.square(divResult)*np.array([0,0,0,1]))\n",
    "\n",
    "def set_model(target_idx):  # 0:x,y, 1:m, 2:v\n",
    "    \n",
    "    activation = 'elu'\n",
    "    padding = 'valid'\n",
    "    model = Sequential()\n",
    "    nf = 16\n",
    "    fs = (3,1)\n",
    "    model.add(Conv2D(nf,fs, padding=padding, activation=activation,input_shape=(X_train.shape[1],X_train.shape[2],X_train.shape[3]))) # # of layers, filter size\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 1))) # 186, 13, 16\n",
    "\n",
    "    model.add(Conv2D(nf*2,fs, padding=padding, activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 1))) #  92, 13, 32\n",
    "\n",
    "    model.add(Conv2D(nf*4,fs, padding=padding, activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 1))) #  45, 13, 64\n",
    "\n",
    "    model.add(Conv2D(nf*8,fs, padding=padding, activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 1))) #  21, 13, 128\n",
    "\n",
    "    model.add(Conv2D(nf*16,fs, padding=padding, activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 1))) # 9, 13, 256\n",
    "\n",
    "    model.add(Conv2D(nf*32,fs, padding=padding, activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 1)))  # 3, 13, 512\n",
    "\n",
    "    \n",
    "    model.add(Flatten()) \n",
    "    model.add(Dense(128, activation ='elu')) # output size만 적어줌\n",
    "    model.add(Dense(64, activation ='elu'))\n",
    "    model.add(Dense(32, activation ='elu'))\n",
    "    model.add(Dense(16, activation ='elu'))\n",
    "    model.add(Dense(4))       \n",
    "\n",
    "    optimizer = keras.optimizers.Adam()\n",
    "\n",
    "    if target_idx==0:\n",
    "        model.compile(loss=my_loss_E1,\n",
    "                  optimizer=optimizer,\n",
    "                     )\n",
    "    elif target_idx==1:\n",
    "        model.compile(loss=my_loss_E2_M,\n",
    "                  optimizer=optimizer,\n",
    "                 )\n",
    "    else:\n",
    "        model.compile(loss=my_loss_E2_V,\n",
    "                  optimizer=optimizer,\n",
    "                 )\n",
    "    return model\n",
    "\n",
    "def train(model, X, Y, is_val=False):\n",
    "    MODEL_SAVE_FOLDER_PATH = './model/'\n",
    "    if not os.path.exists(MODEL_SAVE_FOLDER_PATH):\n",
    "        os.mkdir(MODEL_SAVE_FOLDER_PATH)\n",
    "\n",
    "    model_path = MODEL_SAVE_FOLDER_PATH + '{epoch:02d}-{val_loss:.4f}.hdf5'\n",
    "    best_save = ModelCheckpoint('best_m.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "\n",
    "    if is_val == False:\n",
    "        history = model.fit(X, Y,\n",
    "                      epochs=100,\n",
    "                      batch_size=256,\n",
    "                      shuffle=True,\n",
    "                      validation_split=0.2,\n",
    "                      verbose = 0,\n",
    "                      callbacks=[best_save])\n",
    "\n",
    "        fig, loss_ax = plt.subplots()\n",
    "        acc_ax = loss_ax.twinx()\n",
    "\n",
    "        loss_ax.plot(history.history['loss'], 'y', label='train loss')\n",
    "        loss_ax.plot(history.history['val_loss'], 'r', label='val loss')\n",
    "        loss_ax.set_xlabel('epoch')\n",
    "        loss_ax.set_ylabel('loss')\n",
    "        loss_ax.legend(loc='upper left')\n",
    "        plt.show()    \n",
    "        \n",
    "    else:\n",
    "        history = model.fit(X, Y,\n",
    "                      epochs=100,\n",
    "                      batch_size=256,\n",
    "                      shuffle=True,\n",
    "                      validation_split=0.2,\n",
    "                      verbose = 0,\n",
    "                      callbacks=[best_save])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def load_best_model(target_idx):\n",
    "\n",
    "    if target_idx == 0:\n",
    "        model = load_model('best_m.hdf5' , custom_objects={'my_loss_E1': my_loss, })\n",
    "    elif target_idx == 1:\n",
    "        model = load_model('best_m.hdf5' , custom_objects={'my_loss_E2_M': my_loss, })\n",
    "    else:\n",
    "        model = load_model('best_m.hdf5' , custom_objects={'my_loss_E2_V': my_loss, })\n",
    "\n",
    "    score = model.evaluate(X_train, Y_train, verbose=0) \n",
    "    print('loss:', score)\n",
    "\n",
    "    return model\n",
    "\n",
    "for target_idx in [0,1,2]: # 학습 순서 조정 \n",
    "    model = set_model(target_idx)\n",
    "    train(model,X_train, Y_train)    \n",
    "    best_model = load_best_model(target_idx)\n",
    "    pred = best_model.predict(X_test)\n",
    "    \n",
    "    if target_idx == 0: # x, y 학습\n",
    "        sample_submission.iloc[:,1] = pred[:,0]\n",
    "        sample_submission.iloc[:,2] = pred[:,1]\n",
    "#         X_train = np.concatenate([X_train, np.reshape(np.repeat(Y_train[:,0], 375), (2800,375,1,1))], axis=2) # X 설명변수로 추가\n",
    "#         X_test = np.concatenate([X_test, np.reshape(np.repeat(pred[:,0], 375), (700,375,1,1))], axis=2)\n",
    "\n",
    "    elif target_idx == 1: # m 학습\n",
    "        sample_submission.iloc[:,3] = pred[:,2]\n",
    "\n",
    "    elif target_idx == 2: # v 학습\n",
    "        sample_submission.iloc[:,4] = pred[:,3]\n",
    "\n",
    "result3 = sample_submission.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "sample_submission = pd.read_csv('/kaggle/input/dacon-crash-prediction/sample_submission.csv')\n",
    "\n",
    "master = JB_applying(train)\n",
    "master = JB2_applying(master)\n",
    "master = arr_time(master)\n",
    "master = checking_convex(master)\n",
    "master = AT_diff(master)\n",
    "\n",
    "test_master = JB_applying(test)\n",
    "test_master = JB2_applying(test_master)\n",
    "test_master = arr_time(test_master)\n",
    "test_master = checking_convex(test_master)\n",
    "test_master = AT_diff(test_master)\n",
    "\n",
    "# reshape\n",
    "master_np = np.reshape(np.array(master)[:,1:], (2800, 375, master.shape[1]-1, 1)) # id 컬럼 생략\n",
    "test_master_np = np.reshape(np.array(test_master)[:,1:], (700, 375, test_master.shape[1]-1, 1))\n",
    "\n",
    "X_train = master_np.copy()\n",
    "Y_train = np.array(train_target)[:,1:]\n",
    "X_test = test_master_np.copy()\n",
    "\n",
    "print(X_train.shape, np.isnan(X_train).sum().sum())\n",
    "print(Y_train.shape, np.isnan(Y_train).sum().sum())\n",
    "print(X_test.shape, np.isnan(X_test).sum().sum())\n",
    "\n",
    "def my_loss(y_true, y_pred):\n",
    "    divResult = Lambda(lambda x: x[0]/x[1])([(y_pred-y_true),(y_true+0.000001)])\n",
    "    return K.mean(K.square(divResult))\n",
    "\n",
    "def my_loss_E1(y_true, y_pred):\n",
    "    return K.mean(K.square(y_true-y_pred)*np.array([1,1,0,0]))/2e+04\n",
    "\n",
    "def my_loss_E2_M(y_true, y_pred):\n",
    "    divResult = Lambda(lambda x: x[0]/x[1])([(y_pred-y_true),(y_true+0.000001)])\n",
    "    return K.mean(K.square(divResult)*np.array([0,0,1,0]))\n",
    "\n",
    "def my_loss_E2_V(y_true, y_pred):\n",
    "    divResult = Lambda(lambda x: x[0]/x[1])([(y_pred-y_true),(y_true+0.000001)])\n",
    "    return K.mean(K.square(divResult)*np.array([0,0,0,1]))\n",
    "\n",
    "def set_model(target_idx):  # 0:x,y, 1:m, 2:v\n",
    "    \n",
    "    activation = 'elu'\n",
    "    padding = 'valid'\n",
    "    model = Sequential()\n",
    "    nf = 16\n",
    "    fs = (3,1)\n",
    "    model.add(Conv2D(nf,fs, padding=padding, activation=activation,input_shape=(X_train.shape[1],X_train.shape[2],X_train.shape[3]))) # # of layers, filter size\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 1))) # 186, 13, 16\n",
    "\n",
    "    model.add(Conv2D(nf*2,fs, padding=padding, activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 1))) #  92, 13, 32\n",
    "\n",
    "    model.add(Conv2D(nf*4,fs, padding=padding, activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 1))) #  45, 13, 64\n",
    "\n",
    "    model.add(Conv2D(nf*8,fs, padding=padding, activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 1))) #  21, 13, 128\n",
    "\n",
    "    model.add(Conv2D(nf*16,fs, padding=padding, activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 1))) # 9, 13, 256\n",
    "\n",
    "    model.add(Conv2D(nf*32,fs, padding=padding, activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 1)))  # 3, 13, 512\n",
    "\n",
    "    \n",
    "    model.add(Flatten()) \n",
    "    model.add(Dense(128, activation ='elu')) # output size만 적어줌\n",
    "    model.add(Dense(64, activation ='elu'))\n",
    "    model.add(Dense(32, activation ='elu'))\n",
    "    model.add(Dense(16, activation ='elu'))\n",
    "    model.add(Dense(4))       \n",
    "\n",
    "    optimizer = keras.optimizers.Adam()\n",
    "\n",
    "    if target_idx==0:\n",
    "        model.compile(loss=my_loss_E1,\n",
    "                  optimizer=optimizer,\n",
    "                     )\n",
    "    elif target_idx==1:\n",
    "        model.compile(loss=my_loss_E2_M,\n",
    "                  optimizer=optimizer,\n",
    "                 )\n",
    "    else:\n",
    "        model.compile(loss=my_loss_E2_V,\n",
    "                  optimizer=optimizer,\n",
    "                 )\n",
    "    return model\n",
    "\n",
    "def train(model, X, Y, is_val=False):\n",
    "    MODEL_SAVE_FOLDER_PATH = './model/'\n",
    "    if not os.path.exists(MODEL_SAVE_FOLDER_PATH):\n",
    "        os.mkdir(MODEL_SAVE_FOLDER_PATH)\n",
    "\n",
    "    model_path = MODEL_SAVE_FOLDER_PATH + '{epoch:02d}-{val_loss:.4f}.hdf5'\n",
    "    best_save = ModelCheckpoint('best_m.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "\n",
    "    if is_val == False:\n",
    "        history = model.fit(X, Y,\n",
    "                      epochs=150,\n",
    "                      batch_size=256,\n",
    "                      shuffle=True,\n",
    "                      validation_split=0.2,\n",
    "                      verbose = 0,\n",
    "                      callbacks=[best_save])\n",
    "\n",
    "        fig, loss_ax = plt.subplots()\n",
    "        acc_ax = loss_ax.twinx()\n",
    "\n",
    "        loss_ax.plot(history.history['loss'], 'y', label='train loss')\n",
    "        loss_ax.plot(history.history['val_loss'], 'r', label='val loss')\n",
    "        loss_ax.set_xlabel('epoch')\n",
    "        loss_ax.set_ylabel('loss')\n",
    "        loss_ax.legend(loc='upper left')\n",
    "        plt.show()    \n",
    "        \n",
    "    else:\n",
    "        history = model.fit(X, Y,\n",
    "                      epochs=150,\n",
    "                      batch_size=256,\n",
    "                      shuffle=True,\n",
    "                      validation_split=0.2,\n",
    "                      verbose = 0,\n",
    "                      callbacks=[best_save])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def load_best_model(target_idx):\n",
    "\n",
    "    if target_idx == 0:\n",
    "        model = load_model('best_m.hdf5' , custom_objects={'my_loss_E1': my_loss, })\n",
    "    elif target_idx == 1:\n",
    "        model = load_model('best_m.hdf5' , custom_objects={'my_loss_E2_M': my_loss, })\n",
    "    else:\n",
    "        model = load_model('best_m.hdf5' , custom_objects={'my_loss_E2_V': my_loss, })\n",
    "\n",
    "    score = model.evaluate(X_train, Y_train, verbose=0) \n",
    "    print('loss:', score)\n",
    "\n",
    "    return model\n",
    "\n",
    "for target_idx in [0,1,2]: # 학습 순서 조정 \n",
    "    model = set_model(target_idx)\n",
    "    train(model,X_train, Y_train)    \n",
    "    best_model = load_best_model(target_idx)\n",
    "    pred = best_model.predict(X_test)\n",
    "    \n",
    "    if target_idx == 0: # x, y 학습\n",
    "        sample_submission.iloc[:,1] = pred[:,0]\n",
    "        sample_submission.iloc[:,2] = pred[:,1]\n",
    "#         X_train = np.concatenate([X_train, np.reshape(np.repeat(Y_train[:,0], 375), (2800,375,1,1))], axis=2) # X 설명변수로 추가\n",
    "#         X_test = np.concatenate([X_test, np.reshape(np.repeat(pred[:,0], 375), (700,375,1,1))], axis=2)\n",
    "\n",
    "    elif target_idx == 1: # m 학습\n",
    "        sample_submission.iloc[:,3] = pred[:,2]\n",
    "\n",
    "    elif target_idx == 2: # v 학습\n",
    "        sample_submission.iloc[:,4] = pred[:,3]\n",
    "\n",
    "result4 = sample_submission.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('/kaggle/input/dacon-crash-prediction/sample_submission.csv')\n",
    "\n",
    "sample_submission[['X', 'Y', 'M' 'V']] = result1[['X', 'Y', 'M' 'V']]*0.8 + result2[['X', 'Y', 'M' 'V']]*0.07 + result3[['X', 'Y', 'M' 'V']]*0.07 + result4[['X', 'Y', 'M' 'V']]*0.06 \n",
    "sample_submission.to_csv('submission.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

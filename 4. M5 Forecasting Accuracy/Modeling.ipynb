{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"is_test = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CAL_DTYPES={\"event_name_1\": \"category\", \"event_name_2\": \"category\", \"event_type_1\": \"category\", \n         \"event_type_2\": \"category\", \"weekday\": \"category\", 'wm_yr_wk': 'int16', \"wday\": \"int16\",\n        \"month\": \"int16\", \"year\": \"int16\", \"snap_CA\": \"float32\", 'snap_TX': 'float32', 'snap_WI': 'float32' }\nPRICE_DTYPES = {\"store_id\": \"category\", \"item_id\": \"category\", \"wm_yr_wk\": \"int16\",\"sell_price\":\"float32\" }\nSALES_DTYPES = {'id':\"category\", 'item_id':\"category\", \"dept_id\":\"category\", \"cat_id\":\"category\", \"state_id\":\"category\"}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"calender = pd.read_csv(\"/kaggle/input/m5-forecasting-accuracy/calendar.csv\", dtype = CAL_DTYPES)\nsell_price = pd.read_csv(\"/kaggle/input/m5-forecasting-accuracy/sell_prices.csv\", dtype = PRICE_DTYPES)\nsales_train_val = pd.read_csv(\"/kaggle/input/m5-forecasting-accuracy/sales_train_validation.csv\", dtype=SALES_DTYPES)\nsmp_submission = pd.read_csv(\"/kaggle/input/m5-forecasting-accuracy/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train_val.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 전처리","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# sales_train_val\nkey = sales_train_val['item_id'].astype(str) + '_' + sales_train_val['store_id'].astype(str)\nsales_train_val = sales_train_val.iloc[:,6:]\nsales_train_val.columns = list(map(lambda x: dict(zip(calender['d'], calender['date']))[x], sales_train_val.columns))\nsales_train_val = sales_train_val.iloc[:,-1395:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in sales_train_val.columns:\n    sales_train_val[col] = sales_train_val[col].astype(np.int16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"calender['is_weekend'] = calender['weekday'].apply(lambda x: 1 if (x=='Saturday')or(x=='Sunday') else 0)\ncalender['holi_event'] = calender['event_name_1'].apply(lambda x: 1 if not pd.isnull(x) and x in ['PresidentDay', 'MemorialDay', 'IndependenceDay', 'LaborDay', 'ColumnbusDay', 'VeteransDay', 'Thanksgiving', 'Christmas', 'NewYear', 'MartinLutherKingDay'] else 0)\ncalender['is_event'] = calender['event_name_1'].apply(lambda x: 1 if not pd.isnull(x) else 0)\ncalender.drop(['weekday', 'd', 'event_name_1', 'event_type_1','event_name_2','event_type_2'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sell_price['key'] = sell_price['item_id'].astype(str) + '_' + sell_price['store_id'].astype(str)\nsell_price['key'] = sell_price['key'].astype('category')\nsell_price.drop(['store_id', 'item_id'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"my_valset_cal = calender[1885:1913].drop('date', axis=1)\nvalset_cal = calender[1913:1941].drop('date', axis=1)\ntestset_cal =calender[1941:].drop('date', axis=1)\nmy_valset_y = sales_train_val.iloc[:,-28:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import datetime\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if is_test:\n    sales_train_val = sales_train_val.iloc[:,:100]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import defaultdict\n\nn_iter = 1\nval_test_final = np.zeros((len(key)*2, 28))\nmy_val_final = np.zeros((len(key), 28))\nfor i in range(n_iter):\n    n_clusters = 1\n    km = KMeans(n_clusters = n_clusters, random_state = i)\n    cluster = km.fit_predict(sales_train_val)\n\n    cols_order = []\n    val_result = []\n    test_result = []\n    my_val_result = []\n    print('Start', datetime.datetime.now())\n    for i in range(n_clusters): # 'date' 제외\n        # for my validation\n        tmp = sales_train_val.copy()\n        sales_train_val = sales_train_val.iloc[:,:-28]\n        trainset = pd.DataFrame({'key': np.repeat(key[cluster==i], len(sales_train_val.columns)),\n                                 'date': list(sales_train_val.columns)*sum(cluster==i),\n                                'sales': np.hstack(np.array(sales_train_val)[cluster==i])})\\\n                    .merge(calender, how='left', on='date').merge(sell_price, how='left', on=['key', 'wm_yr_wk']).fillna(-1)\n        sell_price_tmp = sell_price[['wm_yr_wk', 'sell_price']][sell_price['key'].isin(key[cluster==i])]\n        sell_price_tmp['sort_key'] = list(range(len(sell_price_tmp)))\n        my_valset = my_valset_cal.merge(sell_price_tmp, how='left', on= 'wm_yr_wk').sort_values(['sort_key', 'wm_yr_wk', 'wday']).drop('sort_key', axis=1)\n\n        trainset.drop(['key', 'date'], axis=1, inplace=True)\n        trainset.fillna(-1, inplace=True)\n        \n        lgbm = lgb.LGBMRegressor(objective = \"poisson\",\n                                metric = \"rmse\",\n                                force_row_wise = True,\n                                learning_rate = 0.075,\n                                sub_row = 0.75,\n                                bagging_freq = 1,\n                                lambda_l2 = 0.1,\n                                metri= [\"rmse\"],\n                                verbosit=1,\n                                num_iterations = 1200,\n                                num_leaves= 128,\n                                min_data_in_leaf= 100,\n                                n_jobs = -1,\n                                random_state=5)\n        lgbm.fit(X=trainset.drop('sales', axis=1), y=trainset['sales'])    \n        my_val_result.append(lgbm.predict(my_valset))\n\n        # for the leaderboard\n        sales_train_val = tmp.copy()\n        trainset = pd.DataFrame({'key': np.repeat(key[cluster==i], len(sales_train_val.columns)),\n                                 'date': list(sales_train_val.columns)*sum(cluster==i),\n                                'sales': np.hstack(np.array(sales_train_val)[cluster==i])})\\\n                    .merge(calender, how='left', on='date').merge(sell_price, how='left', on=['key', 'wm_yr_wk']).fillna(-1)\n        sell_price_tmp = sell_price[['wm_yr_wk', 'sell_price']][sell_price['key'].isin(key[cluster==i])]\n        sell_price_tmp['sort_key'] = list(range(len(sell_price_tmp)))\n        valset = valset_cal.merge(sell_price_tmp, how='left', on= 'wm_yr_wk').sort_values(['sort_key', 'wm_yr_wk', 'wday']).drop('sort_key', axis=1)\n        testset = testset_cal.merge(sell_price_tmp, how='left', on= 'wm_yr_wk').sort_values(['sort_key', 'wm_yr_wk', 'wday']).drop('sort_key', axis=1)\n\n        trainset.drop(['key', 'date'], axis=1, inplace=True)\n        trainset.fillna(-1, inplace=True)\n\n        lgbm.fit(X=trainset.drop('sales', axis=1), y=trainset['sales'])\n        val_result.append(lgbm.predict(valset))\n        test_result.append(lgbm.predict(testset))\n\n        cols_order.append(key[cluster==i])\n        if i%10 ==0:\n            print(i, 'th Item Done. ----- Trainset Shape: {}'.format(trainset.shape))\n    print('End', datetime.datetime.now())\n    \n    # my_val\n    result = defaultdict(list)\n    my_val_result = np.hstack(my_val_result)\n    for i, col in enumerate(np.hstack(cols_order)):\n        result[col]=my_val_result[i*28:(i+1)*28]\n\n    my_val_final_ = []\n    for i, col in enumerate(key):\n        my_val_final_.append(result[col])\n        \n    my_val_final += np.array(my_val_final_)\n    \n    # val, test\n    result = []\n    val_result = np.hstack(val_result)\n    test_result = np.hstack(test_result)\n    for i, col in enumerate(np.hstack(cols_order)):\n        result.append([col+'_validation'] + list(val_result[i*28:(i+1)*28]))\n        result.append([col+'_evaluation'] + list(test_result[i*28:(i+1)*28]))\n    result = pd.DataFrame(result)\n    result.columns = ['id'] + ['F'+str(i) for i in range(1,29)]\n    val_test_final += np.array(smp_submission[['id']].merge(result, how='left', on='id').drop('id', axis=1))\n    \nmy_val_final /= n_iter\nval_test_final /= n_iter","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from typing import Union\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm_notebook as tqdm\n\n\nclass WRMSSEEvaluator(object):\n\n    def __init__(self, train_df: pd.DataFrame, valid_df: pd.DataFrame, calendar: pd.DataFrame, prices: pd.DataFrame):\n        train_y = train_df.loc[:, train_df.columns.str.startswith('d_')]\n        train_target_columns = train_y.columns.tolist()\n        weight_columns = train_y.iloc[:, -28:].columns.tolist()\n\n        train_df['all_id'] = 0  # for lv1 aggregation\n\n        id_columns = train_df.loc[:, ~train_df.columns.str.startswith('d_')].columns.tolist()\n        valid_target_columns = valid_df.loc[:, valid_df.columns.str.startswith('d_')].columns.tolist()\n\n        if not all([c in valid_df.columns for c in id_columns]):\n            valid_df = pd.concat([train_df[id_columns], valid_df], axis=1, sort=False)\n\n        self.train_df = train_df\n        self.valid_df = valid_df\n        self.calendar = calendar\n        self.prices = prices\n\n        self.weight_columns = weight_columns\n        self.id_columns = id_columns\n        self.valid_target_columns = valid_target_columns\n\n        weight_df = self.get_weight_df()\n\n        self.group_ids = (\n            'all_id',\n            'state_id',\n            'store_id',\n            'cat_id',\n            'dept_id',\n            ['state_id', 'cat_id'],\n            ['state_id', 'dept_id'],\n            ['store_id', 'cat_id'],\n            ['store_id', 'dept_id'],\n            'item_id',\n            ['item_id', 'state_id'],\n            ['item_id', 'store_id']\n        )\n\n        for i, group_id in enumerate(tqdm(self.group_ids)):\n            train_y = train_df.groupby(group_id)[train_target_columns].sum()\n            scale = []\n            for _, row in train_y.iterrows():\n                series = row.values[np.argmax(row.values != 0):]\n                scale.append(((series[1:] - series[:-1]) ** 2).mean())\n            setattr(self, f'lv{i + 1}_scale', np.array(scale))\n            setattr(self, f'lv{i + 1}_train_df', train_y)\n            setattr(self, f'lv{i + 1}_valid_df', valid_df.groupby(group_id)[valid_target_columns].sum())\n\n            lv_weight = weight_df.groupby(group_id)[weight_columns].sum().sum(axis=1)\n            setattr(self, f'lv{i + 1}_weight', lv_weight / lv_weight.sum())\n\n    def get_weight_df(self) -> pd.DataFrame:\n        day_to_week = self.calendar.set_index('d')['wm_yr_wk'].to_dict()\n        weight_df = self.train_df[['item_id', 'store_id'] + self.weight_columns].set_index(['item_id', 'store_id'])\n        weight_df = weight_df.stack().reset_index().rename(columns={'level_2': 'd', 0: 'value'})\n        weight_df['wm_yr_wk'] = weight_df['d'].map(day_to_week)\n\n        weight_df = weight_df.merge(self.prices, how='left', on=['item_id', 'store_id', 'wm_yr_wk'])\n        weight_df['value'] = weight_df['value'] * weight_df['sell_price']\n        weight_df = weight_df.set_index(['item_id', 'store_id', 'd']).unstack(level=2)['value']\n        weight_df = weight_df.loc[zip(self.train_df.item_id, self.train_df.store_id), :].reset_index(drop=True)\n        weight_df = pd.concat([self.train_df[self.id_columns], weight_df], axis=1, sort=False)\n        return weight_df\n\n    def rmsse(self, valid_preds: pd.DataFrame, lv: int) -> pd.Series:\n        valid_y = getattr(self, f'lv{lv}_valid_df')\n        score = ((valid_y - valid_preds) ** 2).mean(axis=1)\n        scale = getattr(self, f'lv{lv}_scale')\n        return (score / scale).map(np.sqrt)\n\n    def score(self, valid_preds: Union[pd.DataFrame, np.ndarray]) -> float:\n        assert self.valid_df[self.valid_target_columns].shape == valid_preds.shape\n\n        if isinstance(valid_preds, np.ndarray):\n            valid_preds = pd.DataFrame(valid_preds, columns=self.valid_target_columns)\n\n        valid_preds = pd.concat([self.valid_df[self.id_columns], valid_preds], axis=1, sort=False)\n\n        all_scores = []\n        for i, group_id in enumerate(self.group_ids):\n            lv_scores = self.rmsse(valid_preds.groupby(group_id)[self.valid_target_columns].sum(), i + 1)\n            weight = getattr(self, f'lv{i + 1}_weight')\n            lv_scores = pd.concat([weight, lv_scores], axis=1, sort=False).prod(axis=1)\n            all_scores.append(lv_scores.sum())\n\n        return np.mean(all_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df =  pd.read_csv(\"/kaggle/input/m5-forecasting-accuracy/sales_train_validation.csv\")\ntrain_fold_df = train_df.iloc[:, :-28]\nvalid_fold_df = train_df.iloc[:, -28:]\nvalid_preds = pd.DataFrame(my_val_final)\nvalid_preds.columns = valid_fold_df.columns\n\nevaluator = WRMSSEEvaluator(train_fold_df, valid_fold_df, \n                            pd.read_csv(\"/kaggle/input/m5-forecasting-accuracy/calendar.csv\"), \n                            pd.read_csv(\"/kaggle/input/m5-forecasting-accuracy/sell_prices.csv\"))\nevaluator.score(valid_preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\n_ = plt.plot(valid_preds.T.iloc[:,np.random.choice(30490, 100, replace=False)], color='blue', alpha=0.3)\n_ = plt.plot(valid_fold_df.T.iloc[:,np.random.choice(30490, 100, replace=False)], color='red', alpha=0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\n_ = plt.plot(valid_preds.T.iloc[:,np.random.choice(30490, 100, replace=False)], color='blue', alpha=0.3)\n_ = plt.plot(valid_fold_df.T.iloc[:,np.random.choice(30490, 100, replace=False)], color='red', alpha=0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\n_ = plt.plot(valid_preds.T.iloc[:,np.random.choice(30490, 100, replace=False)], color='blue', alpha=0.3)\n_ = plt.plot(valid_fold_df.T.iloc[:,np.random.choice(30490, 100, replace=False)], color='red', alpha=0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\n_ = plt.plot(valid_preds.T.mean(axis=1), color='blue')\n_ = plt.plot(valid_fold_df.T.mean(axis=1), color='red')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# submission data building\n# 여기도 앙상블 과정 만들어야함 \n\nsubmission = pd.concat([pd.DataFrame(np.concatenate([pd.Series(key)+'_validation', pd.Series(key)+'_evaluation'])), pd.DataFrame(val_test_final)], axis=1)\nsubmission.columns = ['id'] + ['F'+str(i) for i in range(1,29)]\nsubmission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}